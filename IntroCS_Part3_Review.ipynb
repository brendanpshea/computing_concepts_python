{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO804YnPc/fGRLVEGRgM6d3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendanpshea/computing_concepts_python/blob/main/IntroCS_Part3_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdwVFVIiRjjt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Review: Databases, Data Structures, AI"
      ],
      "metadata": {
        "id": "jRMF6FOdRkLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a relational database?\n",
        "\n",
        "A **relational database** is a type of database that organizes data into tables, with each table consisting of rows (also known as records or tuples) and columns (also known as fields or attributes). The tables are related to each other through **primary keys** and **foreign keys**, which establish relationships between the tables and allow for efficient data retrieval and manipulation.\n",
        "\n",
        "In a relational database, each table represents a specific entity or concept, and each row within a table represents a unique instance of that entity. The columns define the attributes or properties of the entity. The relationships between tables are established based on the **logical connections** between the entities they represent.\n",
        "\n",
        "**Relational databases** are based on the **relational model**, which was introduced by Edgar F. Codd in 1970. The relational model provides a mathematical foundation for organizing and manipulating data in a structured manner. It defines concepts such as **data integrity**, **normalization**, and **relational algebra**, which are essential for maintaining data consistency and performing complex queries.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's consider a relational database for a library system. The database might have tables such as \"Books,\" \"Authors,\" and \"Borrowers.\" The \"Books\" table could have columns like \"BookID\" (primary key), \"Title,\" \"AuthorID\" (foreign key referencing the \"Authors\" table), and \"PublicationYear.\" The \"Authors\" table could have columns like \"AuthorID\" (primary key) and \"AuthorName.\" The \"Borrowers\" table could have columns like \"BorrowerID\" (primary key), \"BorrowerName,\" and \"BookID\" (foreign key referencing the \"Books\" table).\n",
        "\n",
        "In this example, the relationships between the tables are established through the foreign keys. The \"AuthorID\" in the \"Books\" table relates each book to its corresponding author in the \"Authors\" table, while the \"BookID\" in the \"Borrowers\" table relates each borrowed book to its corresponding book in the \"Books\" table. This allows for efficient querying and retrieval of data, such as finding all books written by a specific author or identifying the borrowers of a particular book."
      ],
      "metadata": {
        "id": "71yzDLZgRofY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Books Table:\n",
        "\n",
        "| BookID | Title | AuthorID | PublicationYear |\n",
        "| --- | --- | --- | --- |\n",
        "| 1 | Ulysses | 1 | 1922 |\n",
        "| 2 | Dubliners | 1 | 1914 |\n",
        "| 3 | The Importance of Being Earnest | 2 | 1895 |\n",
        "| 4 | The Picture of Dorian Gray | 2 | 1890 |\n",
        "| 5 | The Playboy of the Western World | 3 | 1907 |\n",
        "\n",
        "#### Authors Table:\n",
        "\n",
        "| AuthorID | AuthorName |\n",
        "| --- | --- |\n",
        "| 1 | James Joyce |\n",
        "| 2 | Oscar Wilde |\n",
        "| 3 | John Millington Synge |\n",
        "\n",
        "#### Borrowers Table:\n",
        "\n",
        "| BorrowerID | BorrowerName | BookID |\n",
        "| --- | --- | --- |\n",
        "| 1 | John Smith | 1 |\n",
        "| 2 | Emma Johnson | 2 |\n",
        "| 3 | Michael Brown | 1 |\n",
        "| 4 | Sarah Davis | 3 |\n",
        "| 5 | David Wilson | 4 |\n",
        "\n",
        "In this example:\n",
        "\n",
        "-   The \"Books\" table contains information about each book, including its unique identifier (BookID), title, author (referenced by AuthorID), and publication year.\n",
        "-   The \"Authors\" table stores the details of each author, with their unique identifier (AuthorID) and name.\n",
        "-   The \"Borrowers\" table keeps track of who has borrowed which book, using the BorrowerID and BookID as foreign keys to establish the relationship between borrowers and books.\n",
        "\n",
        "The relationships between the tables are as follows:\n",
        "\n",
        "-   The \"AuthorID\" in the \"Books\" table is a foreign key referencing the \"AuthorID\" in the \"Authors\" table, establishing the relationship between books and their authors.\n",
        "-   The \"BookID\" in the \"Borrowers\" table is a foreign key referencing the \"BookID\" in the \"Books\" table, establishing the relationship between borrowed books and their corresponding book records.\n",
        "\n",
        "These tables and their relationships allow for efficient querying and data retrieval, such as finding all books written by a specific author or identifying the borrowers of a particular book."
      ],
      "metadata": {
        "id": "OPtWfka_Rtzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext sql\n",
        "%sql sqlite://"
      ],
      "metadata": {
        "id": "cz1DGDzjTNb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Load SQL extension and establish a connection to an SQLite memory-based database\n",
        "\n",
        "-- Creating the Authors table\n",
        "CREATE TABLE Authors (\n",
        "    AuthorID INTEGER PRIMARY KEY,\n",
        "    AuthorName TEXT NOT NULL\n",
        ");\n",
        "\n",
        "-- Inserting data into the Authors table\n",
        "INSERT INTO Authors (AuthorID, AuthorName) VALUES\n",
        "    (1, 'James Joyce'),\n",
        "    (2, 'Oscar Wilde'),\n",
        "    (3, 'John Millington Synge');\n",
        "\n",
        "-- Creating the Books table\n",
        "CREATE TABLE Books (\n",
        "    BookID INTEGER PRIMARY KEY,\n",
        "    Title TEXT NOT NULL,\n",
        "    AuthorID INTEGER,\n",
        "    PublicationYear INTEGER,\n",
        "    FOREIGN KEY (AuthorID) REFERENCES Authors(AuthorID)\n",
        ");\n",
        "\n",
        "-- Inserting data into the Books table\n",
        "INSERT INTO Books (BookID, Title, AuthorID, PublicationYear) VALUES\n",
        "    (1, 'Ulysses', 1, 1922),\n",
        "    (2, 'Dubliners', 1, 1914),\n",
        "    (3, 'The Importance of Being Earnest', 2, 1895),\n",
        "    (4, 'The Picture of Dorian Gray', 2, 1890),\n",
        "    (5, 'The Playboy of the Western World', 3, 1907);\n",
        "\n",
        "-- Creating the Borrowers table\n",
        "CREATE TABLE Borrowers (\n",
        "    BorrowerID INTEGER PRIMARY KEY,\n",
        "    BorrowerName TEXT NOT NULL,\n",
        "    BookID INTEGER,\n",
        "    FOREIGN KEY (BookID) REFERENCES Books(BookID)\n",
        ");\n",
        "\n",
        "-- Inserting data into the Borrowers table\n",
        "INSERT INTO Borrowers (BorrowerID, BorrowerName, BookID) VALUES\n",
        "    (1, 'John Smith', 1),\n",
        "    (2, 'Emma Johnson', 2),\n",
        "    (3, 'Michael Brown', 1),\n",
        "    (4, 'Sarah Davis', 3),\n",
        "    (5, 'David Wilson', 4);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXJ-bR84UalP",
        "outputId": "8321f24f-5f45-4677-de30-d1065999e300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * sqlite://\n",
            "Done.\n",
            "3 rows affected.\n",
            "Done.\n",
            "5 rows affected.\n",
            "Done.\n",
            "5 rows affected.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do I use SQL to query databases?\n",
        "\n",
        "**SQL (Structured Query Language)** is a standard language used to interact with relational databases. It allows you to retrieve, manipulate, and manage data stored in the database tables. SQL provides various commands and clauses to perform different types of queries and operations on the database.\n",
        "\n",
        "Here are some commonly used SQL commands:\n",
        "\n",
        "-   **SELECT**: Retrieves data from one or more tables based on specified conditions.\n",
        "-   **INSERT**: Inserts new records into a table.\n",
        "-   **UPDATE**: Modifies existing records in a table.\n",
        "-   **DELETE**: Removes records from a table based on specified conditions.\n",
        "-   **JOIN**: Combines rows from two or more tables based on a related column between them.\n",
        "-   **WHERE**: Filters the result set based on specified conditions.\n",
        "-   **ORDER BY**: Sorts the result set in ascending or descending order based on one or more columns.\n",
        "-   **GROUP BY**: Groups the result set based on one or more columns.\n",
        "-   **HAVING**: Filters the grouped result set based on specified conditions.\n",
        "-   **LIMIT/OFFSET**: Restricts the number of rows returned by a query and allows for pagination.\n",
        "\n",
        "### Example\n",
        "\n",
        "Using the library database from the previous example, let's look at 10 sample SQL queries:\n",
        "\n",
        "| Query | Explanation |\n",
        "| --- | --- |\n",
        "| SELECT * FROM Books; | Retrieve all columns and rows from the \"Books\" table. |\n",
        "| SELECT Title, PublicationYear FROM Books; | Retrieve only the \"Title\" and \"PublicationYear\" columns from the \"Books\" table. |\n",
        "| SELECT * FROM Books WHERE AuthorID = 1; | Retrieve all books written by the author with AuthorID 1. |\n",
        "| SELECT * FROM Books ORDER BY PublicationYear DESC; | Retrieve all books and sort them in descending order based on the \"PublicationYear\" column. |\n",
        "| SELECT * FROM Books JOIN Authors ON Books.AuthorID = Authors.AuthorID; | Retrieve all books along with their corresponding author information by joining the \"Books\" and \"Authors\" tables based on the \"AuthorID\" column. |\n",
        "| SELECT Authors.AuthorName, COUNT(*) as BookCount FROM Authors JOIN Books ON Authors.AuthorID = Books.AuthorID GROUP BY Authors.AuthorID; | Retrieve the count of books written by each author by joining the \"Authors\" and \"Books\" tables and grouping the results by \"AuthorID\". |\n",
        "| SELECT * FROM Borrowers WHERE BookID = 1; | Retrieve all borrowers who have borrowed the book with BookID 1. |\n",
        "| INSERT INTO Books (Title, AuthorID, PublicationYear) VALUES ('The Dead', 1, 1914); | Insert a new book record into the \"Books\" table with the specified values. |\n",
        "| UPDATE Books SET PublicationYear = 1915 WHERE BookID = 6; | Update the \"PublicationYear\" of the book with BookID 6 to 1915. |\n",
        "| DELETE FROM Borrowers WHERE BorrowerID = 3; | Delete the borrower record with BorrowerID 3 from the \"Borrowers\" table. |\n",
        "\n",
        "These are just a few examples of how SQL can be used to query and manipulate data in a relational database. SQL provides many more commands and clauses to perform complex queries, data aggregation, and database management tasks."
      ],
      "metadata": {
        "id": "T4RMBua7Stwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Python Dictionary?\n",
        "\n",
        "A **Python dictionary** is a built-in data structure that allows you to store and retrieve data in a key-value format. It is an unordered collection of **key-value pairs**, where each key is unique and associated with a specific value. Dictionaries are also known as **associative arrays**, **hash tables**, or **hash maps** in other programming languages.\n",
        "\n",
        "In a dictionary, the **keys** are used to index and access the corresponding **values**. Keys must be immutable objects like strings, numbers, or tuples, while values can be of any data type, including lists, dictionaries, or even functions. The key-value pairs are enclosed in curly braces `{}`, and each pair is separated by a comma.\n",
        "\n",
        "Here's the general syntax for creating a dictionary:\n",
        "\n",
        "```python\n",
        "dictionary_name = {key1: value1, key2: value2, ..., keyN: valueN}\n",
        "```\n",
        "\n",
        "You can access the value associated with a specific key using square brackets `[]` or the `get()` method. You can also modify, add, or remove key-value pairs using various dictionary methods and operations.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's create a dictionary to store information about famous paintings and their artists:"
      ],
      "metadata": {
        "id": "remg4XOeTsQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paintings = {\n",
        "    \"Mona Lisa\": \"Leonardo da Vinci\",\n",
        "    \"The Starry Night\": \"Vincent van Gogh\",\n",
        "    \"The Persistence of Memory\": \"Salvador Dali\",\n",
        "    \"The Scream\": \"Edvard Munch\",\n",
        "    \"Girl with a Pearl Earring\": \"Johannes Vermeer\"\n",
        "}"
      ],
      "metadata": {
        "id": "72WdBjndTvdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take a look at some common operations."
      ],
      "metadata": {
        "id": "qNaovPtJUG2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store information about famous paintings and their artists\n",
        "paintings = {\n",
        "    \"Mona Lisa\": \"Leonardo da Vinci\",\n",
        "    \"The Starry Night\": \"Vincent van Gogh\",\n",
        "    \"The Persistence of Memory\": \"Salvador Dali\",\n",
        "    \"The Scream\": \"Edvard Munch\",\n",
        "    \"Girl with a Pearl Earring\": \"Johannes Vermeer\"\n",
        "}\n",
        "\n",
        "# Accessing values\n",
        "print(\"Accessing values:\")\n",
        "print(paintings[\"Mona Lisa\"])  # Output: Leonardo da Vinci\n",
        "print(paintings.get(\"The Starry Night\"))  # Output: Vincent van Gogh\n",
        "print()\n",
        "\n",
        "# Modifying values\n",
        "print(\"Modifying values:\")\n",
        "paintings[\"The Scream\"] = \"Edvard Munch (Norwegian)\"\n",
        "print(paintings[\"The Scream\"])  # Output: Edvard Munch (Norwegian)\n",
        "print()\n",
        "\n",
        "# Adding new key-value pairs\n",
        "print(\"Adding new key-value pairs:\")\n",
        "paintings[\"The Birth of Venus\"] = \"Sandro Botticelli\"\n",
        "print(paintings[\"The Birth of Venus\"])  # Output: Sandro Botticelli\n",
        "print()\n",
        "\n",
        "# Removing key-value pairs\n",
        "print(\"Removing key-value pairs:\")\n",
        "del paintings[\"The Persistence of Memory\"]\n",
        "print(\"The Persistence of Memory\" in paintings)  # Output: False\n",
        "print()\n",
        "\n",
        "# Checking if a key exists\n",
        "print(\"Checking if a key exists:\")\n",
        "if \"Girl with a Pearl Earring\" in paintings:\n",
        "    print(\"The painting 'Girl with a Pearl Earring' exists in the dictionary.\")\n",
        "print()\n",
        "\n",
        "# Iterating over a dictionary\n",
        "print(\"Iterating over a dictionary:\")\n",
        "for painting, artist in paintings.items():\n",
        "    print(f\"{painting} is painted by {artist}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KKFXvQ3UOdW",
        "outputId": "b9c89438-fb0b-4424-d423-db60f0fc8c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accessing values:\n",
            "Leonardo da Vinci\n",
            "Vincent van Gogh\n",
            "\n",
            "Modifying values:\n",
            "Edvard Munch (Norwegian)\n",
            "\n",
            "Adding new key-value pairs:\n",
            "Sandro Botticelli\n",
            "\n",
            "Removing key-value pairs:\n",
            "False\n",
            "\n",
            "Checking if a key exists:\n",
            "The painting 'Girl with a Pearl Earring' exists in the dictionary.\n",
            "\n",
            "Iterating over a dictionary:\n",
            "Mona Lisa is painted by Leonardo da Vinci.\n",
            "The Starry Night is painted by Vincent van Gogh.\n",
            "The Scream is painted by Edvard Munch (Norwegian).\n",
            "Girl with a Pearl Earring is painted by Johannes Vermeer.\n",
            "The Birth of Venus is painted by Sandro Botticelli.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Object-Oriented Programming? How does it work in Python?\n",
        "\n",
        "**Object-Oriented Programming (OOP)** is a programming paradigm that organizes code into objects, which are instances of classes. It focuses on creating reusable and modular code by encapsulating data and behavior into objects. OOP promotes concepts such as encapsulation, inheritance, and polymorphism, which help in writing more maintainable and scalable code.\n",
        "\n",
        "In Python, everything is an object, and classes are used to define the structure and behavior of objects. A **class** is a blueprint or template that defines the attributes (data) and methods (functions) that an object of that class will have. Objects are created from classes and can interact with each other through their methods.\n",
        "\n",
        "Here are the key concepts of OOP in Python:\n",
        "\n",
        "1.  **Class**: A class is a blueprint for creating objects. It defines the attributes and methods that the objects of the class will possess.\n",
        "2.  **Object**: An object is an instance of a class. It is created from a class and has its own unique set of attributes and can perform actions defined by the class methods.\n",
        "3.  **Encapsulation**: Encapsulation is the process of bundling data and methods into a single unit (object) and restricting direct access to the internal state of the object from outside the class. It provides data protection and promotes modularity.\n",
        "4.  **Inheritance**: Inheritance allows a class to inherit attributes and methods from another class. It promotes code reuse and helps in creating specialized classes based on existing ones.\n",
        "5.  **Polymorphism**: Polymorphism allows objects of different classes to be treated as objects of a common class. It enables writing more flexible and generic code that can work with objects of different types.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's create a simple class hierarchy to represent albums by Minnesota musicians:"
      ],
      "metadata": {
        "id": "LaSdWGHEVDCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Album:\n",
        "    def __init__(self, title, artist, year):\n",
        "        # Constructor method (__init__) initializes an object's attributes\n",
        "        self.title = title  # Assigning the title attribute\n",
        "        self.artist = artist  # Assigning the artist attribute\n",
        "        self.year = year  # Assigning the year attribute\n",
        "\n",
        "    def get_info(self):\n",
        "        # Method to retrieve information about the album\n",
        "        return f\"{self.title} by {self.artist} ({self.year})\"\n",
        "\n",
        "class StudioAlbum(Album):\n",
        "    def __init__(self, title, artist, year, studio):\n",
        "        # Subclass StudioAlbum inherits from superclass Album\n",
        "        super().__init__(title, artist, year)  # Call superclass's constructor\n",
        "        self.studio = studio  # Assigning the studio attribute\n",
        "\n",
        "    def get_info(self):\n",
        "        # Method to retrieve information about the studio album\n",
        "        return f\"{super().get_info()} - Recorded at {self.studio}\"\n",
        "\n",
        "class LiveAlbum(Album):\n",
        "    def __init__(self, title, artist, year, venue):\n",
        "        # Subclass LiveAlbum inherits from superclass Album\n",
        "        super().__init__(title, artist, year)  # Call superclass's constructor\n",
        "        self.venue = venue  # Assigning the venue attribute\n",
        "\n",
        "    def get_info(self):\n",
        "        # Method to retrieve information about the live album\n",
        "        return f\"{super().get_info()} - Recorded live at {self.venue}\"\n"
      ],
      "metadata": {
        "id": "UsGhSdE3VHk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we have a base class called `Album` that represents a generic album with attributes like `title`, `artist`, and `year`. It also has a method `get_info()` that returns a string with the album information.\n",
        "\n",
        "We then have two derived classes, `StudioAlbum` and `LiveAlbum`, which inherit from the `Album` class. They add specific attributes and override the `get_info()` method to provide additional information specific to studio albums and live albums, respectively.\n",
        "\n",
        "Here's how we can create objects from these classes and use them:"
      ],
      "metadata": {
        "id": "fjAeCNIaVTZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating objects of the classes\n",
        "album1 = Album(\"Blonde on Blonde\", \"Bob Dylan\", 1966)\n",
        "album2 = StudioAlbum(\"Purple Rain\", \"Prince\", 1984, \"First Avenue\")\n",
        "album3 = LiveAlbum(\"The Bootleg Series Vol. 4: Bob Dylan Live 1966\", \"Bob Dylan\", 1998, \"Manchester Free Trade Hall\")\n",
        "\n",
        "# Accessing object attributes and methods\n",
        "print(album1.get_info())\n",
        "print(album2.get_info())\n",
        "print(album3.get_info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQLcTH3FVUM8",
        "outputId": "bdf9758d-10de-4b48-9c4d-cdf09cc91a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blonde on Blonde by Bob Dylan (1966)\n",
            "Purple Rain by Prince (1984) - Recorded at First Avenue\n",
            "The Bootleg Series Vol. 4: Bob Dylan Live 1966 by Bob Dylan (1998) - Recorded live at Manchester Free Trade Hall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Principles of Software Engineering\n",
        "\n",
        "Software engineering is the process of designing, developing, testing, and maintaining software systems. It involves applying engineering principles and methodologies to create efficient, reliable, and maintainable software solutions. Two common approaches to software design are the Waterfall model and Agile development.\n",
        "\n",
        "### Waterfall Model\n",
        "\n",
        "The **Waterfall model** is a linear and sequential approach to software development. It consists of distinct phases that are completed one after another, with each phase depending on the deliverables of the previous phase. The phases typically include:\n",
        "\n",
        "1.  Requirements gathering and analysis\n",
        "2.  Design\n",
        "3.  Implementation\n",
        "4.  Testing\n",
        "5.  Deployment\n",
        "6.  Maintenance\n",
        "\n",
        "The Waterfall model emphasizes thorough planning and documentation upfront. It works well for projects with clear and stable requirements, but it can be less flexible when changes are needed during the development process.\n",
        "\n",
        "### Agile Development\n",
        "\n",
        "**Agile development** is an iterative and incremental approach to software development. It focuses on delivering working software quickly and adapting to changing requirements. Agile methodologies, such as Scrum and Kanban, promote collaboration, flexibility, and continuous improvement.\n",
        "\n",
        "In Agile development, the project is divided into small, manageable iterations called sprints. Each sprint typically lasts 1-4 weeks and aims to deliver a potentially shippable product increment. The team works closely with stakeholders and incorporates feedback regularly to ensure the software meets the evolving needs of the users.\n",
        "\n",
        "### Unit Testing\n",
        "\n",
        "**Unit testing** is a software testing technique where individual units or components of the software are tested in isolation to ensure they function correctly. A unit can be a function, method, class, or module. The goal of unit testing is to validate that each unit performs as expected and handles edge cases appropriately.\n",
        "\n",
        "Unit tests are written by developers and are usually automated. They help catch bugs early in the development process, provide confidence in the correctness of the code, and serve as documentation for the expected behavior of the units. Unit testing frameworks, such as Python's `unittest` or JavaScript's Jest, are commonly used to write and run unit tests.\n",
        "\n",
        "### Building Large Software Projects\n",
        "\n",
        "The concepts we've discussed earlier, such as databases, dictionaries, and objects, play crucial roles in constructing large software projects:\n",
        "\n",
        "-   **Databases**: Databases, particularly relational databases, are essential for storing and managing large amounts of structured data. They provide efficient data retrieval, ensure data integrity, and support concurrent access by multiple users. Databases are used in various applications, from e-commerce websites to enterprise systems.\n",
        "-   **Dictionaries**: Dictionaries, or hash tables, are useful data structures for fast data lookup and retrieval based on keys. They are commonly used for caching, indexing, and implementing associative arrays. In large software projects, dictionaries can optimize performance by providing constant-time access to frequently used data.\n",
        "-   **Objects**: Object-oriented programming (OOP) is a fundamental paradigm in software engineering. It allows developers to model real-world entities as objects, encapsulating data and behavior. OOP promotes code reusability, modularity, and maintainability. In large projects, objects help organize code into logical units, making it easier to understand, test, and extend the system.\n",
        "\n",
        "By leveraging these concepts and following software engineering principles, developers can build robust, scalable, and maintainable software systems. Techniques like modular design, version control, continuous integration, and automated testing are also crucial for managing the complexity of large projects and ensuring the quality of the software.\n",
        "\n",
        "Effective software engineering requires a combination of technical skills, problem-solving abilities, and collaboration among team members. It involves understanding the requirements, designing appropriate solutions, writing clean and efficient code, and continuously testing and refining the software to meet the needs of the users."
      ],
      "metadata": {
        "id": "xUB5a4sBWv7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "History and Concepts of Symbolic AI/GOFAI\n",
        "-----------------------------------------\n",
        "\n",
        "Symbolic AI, also known as Good Old-Fashioned Artificial Intelligence (GOFAI), is an approach to artificial intelligence that focuses on using explicit representations of knowledge and formal logic to solve problems. It was the dominant paradigm in AI research from the 1950s to the 1980s.\n",
        "\n",
        "### Formal Logic\n",
        "\n",
        "**Formal logic** is a fundamental concept in Symbolic AI. It involves representing knowledge and reasoning using precise mathematical formulas and rules. The two main types of formal logic used in Symbolic AI are:\n",
        "\n",
        "1.  **Propositional Logic**: Propositional logic deals with statements that can be either true or false. It uses logical connectives like \"and\" (∧), \"or\" (∨), \"not\" (¬), \"implies\" (→), and \"if and only if\" (↔) to combine and manipulate propositions.\n",
        "2.  **First-Order Logic (FOL)**: FOL extends propositional logic by introducing predicates, variables, and quantifiers. It allows for more expressive representations of knowledge and can handle relationships between objects. FOL uses symbols like \"for all\" (∀) and \"there exists\" (∃) to quantify over variables.\n",
        "\n",
        "Formal logic provides a structured way to represent and reason about knowledge in Symbolic AI systems. It allows for logical deduction, inference, and theorem proving.\n",
        "\n",
        "### Regular Expressions\n",
        "\n",
        "**Regular expressions** (regex) are a powerful tool for pattern matching and string manipulation. They are widely used in Symbolic AI for tasks like natural language processing, text parsing, and information extraction.\n",
        "\n",
        "Regular expressions define a search pattern that can match specific sequences of characters within a string. They use special characters and metacharacters to define the pattern. For example:\n",
        "\n",
        "-   `^` matches the start of a string\n",
        "-   `$` matches the end of a string\n",
        "-   `*` matches zero or more occurrences of the preceding character\n",
        "-   `+` matches one or more occurrences of the preceding character\n",
        "-   `?` matches zero or one occurrence of the preceding character\n",
        "-   `[]` defines a character set\n",
        "-   `|` represents an OR condition\n",
        "\n",
        "### Example: C-3PO Chatbot\n",
        "\n",
        "Let's create a simple chatbot based on C-3PO from Star Wars using regular expressions in Python. The chatbot will respond to specific patterns in user input."
      ],
      "metadata": {
        "id": "Q0yyntMSXZms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Import the regex module, which allows for pattern matching in strings\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # This function returns a response from a chatbot based on user input.\n",
        "\n",
        "    # Check if the user's message contains the word \"hello\" (case insensitive)\n",
        "    if re.search(r'\\bhello\\b', user_input, re.IGNORECASE):\n",
        "        # Respond to greeting\n",
        "        return \"Hello! I am C-3PO, human-cyborg relations.\"\n",
        "\n",
        "    # Check if the user's message contains the word \"name\" (case insensitive)\n",
        "    elif re.search(r'\\bname\\b', user_input, re.IGNORECASE):\n",
        "        # Provide the chatbot's name and its capabilities\n",
        "        return \"My name is C-3PO, a protocol droid fluent in over six million forms of communication.\"\n",
        "\n",
        "    # Check if the user's message contains the word \"master\" (case insensitive)\n",
        "    elif re.search(r'\\bmaster\\b', user_input, re.IGNORECASE):\n",
        "        # Mention the chatbot's master\n",
        "        return \"My master is Luke Skywalker, a brave Jedi Knight.\"\n",
        "\n",
        "    # Check if the user's message contains the word \"protocol\" (case insensitive)\n",
        "    elif re.search(r'\\bprotocol\\b', user_input, re.IGNORECASE):\n",
        "        # Explain the function of a protocol droid\n",
        "        return \"As a protocol droid, I am well-versed in etiquette, customs, and translation.\"\n",
        "\n",
        "    # Check if the user's message contains the word \"goodbye\" (case insensitive)\n",
        "    elif re.search(r'\\bgoodbye\\b', user_input, re.IGNORECASE):\n",
        "        # Provide a farewell message\n",
        "        return \"Goodbye! May the Force be with you.\"\n",
        "\n",
        "    # Default response if no recognized keywords are found\n",
        "    else:\n",
        "        return \"I apologize, but I am not quite sure how to respond to that.\"\n",
        "\n",
        "# Initial greeting before entering the loop\n",
        "print(\"C-3PO: Greetings, human! How may I assist you today?\")\n",
        "\n",
        "# Example usage\n",
        "while True:\n",
        "    user_input = input(\"User: \")  # Prompt the user to type something\n",
        "    response = chatbot_response(user_input)  # Generate a response from the chatbot\n",
        "    print(\"C-3PO:\", response)  # Output the chatbot's response\n",
        "\n",
        "    # Check if the conversation should end\n",
        "    if re.search(r'\\bgoodbye\\b', user_input, re.IGNORECASE):\n",
        "        break  # Exit the loop if the user says \"goodbye\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-kzOFuhXa0C",
        "outputId": "37932a40-bbbb-4f62-e48d-2d1aa58cbf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C-3PO: Greetings, human! How may I assist you today?\n",
            "User: goodbye\n",
            "C-3PO: Goodbye! May the Force be with you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `chatbot_response()` function uses regular expressions to match specific patterns in the user input and provides appropriate responses. The regular expressions are case-insensitive and use word boundaries (`\\b`) to ensure precise matching.\n",
        "\n",
        "The chatbot responds to greetings, questions about its name and master, mentions of protocol, and farewell messages. If the user input doesn't match any defined patterns, a default response is provided.\n",
        "\n",
        "The chatbot is run in a loop, allowing the user to interact with it until a goodbye message is entered.\n",
        "\n",
        "Symbolic AI and formal logic laid the foundation for early AI systems and continue to influence research in areas like knowledge representation, reasoning, and natural language processing. Regular expressions, while not exclusively tied to Symbolic AI, are a useful tool for pattern matching and text processing in AI applications."
      ],
      "metadata": {
        "id": "aynsvPPVXnoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Machine Learning\n",
        "While Symbolic AI and formal logic were the dominant approaches in the early days of AI, they faced challenges in dealing with uncertain and noisy real-world data. This led to the emergence of **statistical machine learning**, which focuses on building systems that can learn and make predictions based on data.\n",
        "\n",
        "Statistical machine learning combines principles from statistics, probability theory, and computer science to create algorithms that can automatically learn patterns and relationships from data. Instead of relying on explicitly programmed rules, these algorithms learn from examples and improve their performance over time.\n",
        "\n",
        "### Bayes' Theorem\n",
        "\n",
        "**Bayes' theorem** is a fundamental concept in probability theory and forms the basis for many machine learning algorithms. It describes how to update the probability of a hypothesis (H) given some observed evidence (E). Mathematically, Bayes' theorem is expressed as:\n",
        "\n",
        "P(H|E) = (P(E|H) * P(H)) / P(E)\n",
        "\n",
        "Where:\n",
        "\n",
        "-   P(H|E) is the probability of the hypothesis given the evidence (posterior probability)\n",
        "-   P(E|H) is the probability of the evidence given the hypothesis (likelihood)\n",
        "-   P(H) is the prior probability of the hypothesis\n",
        "-   P(E) is the probability of the evidence\n",
        "\n",
        "Bayes' theorem allows us to incorporate prior knowledge and update our beliefs based on new evidence. It is widely used in machine learning for tasks like classification, spam filtering, and medical diagnosis.\n",
        "\n",
        "### Example: Medical Diagnosis using Bayes' Theorem\n",
        "\n",
        "Let's implement a Python function that calculates the probability of a disease given the presence of a symptom using Bayes' theorem:"
      ],
      "metadata": {
        "id": "jUOobWaPY5m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayes_theorem(pr_h, pr_e_h, pr_e_not_h):\n",
        "    pr_not_h = 1 - pr_h\n",
        "    pr_e = pr_e_h * pr_h + pr_e_not_h * pr_not_h\n",
        "    pr_h_e = (pr_e_h * pr_h) / pr_e\n",
        "    return pr_h_e\n",
        "\n",
        "# Example usage\n",
        "pr_disease = 0.01\n",
        "pr_symptom_given_disease = 0.9\n",
        "pr_symptom_given_no_disease = 0.05\n",
        "\n",
        "pr_disease_given_symptom = bayes_theorem(pr_disease, pr_symptom_given_disease, pr_symptom_given_no_disease)\n",
        "print(f\"Probability of having the disease given the symptom: {pr_disease_given_symptom:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR3QfFXCY6T-",
        "outputId": "0a5134b2-bd2c-4cf1-8a48-4a14d15f4c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of having the disease given the symptom: 0.154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `bayes_theorem` function takes the prior probability of the hypothesis (`pr_h`), the probability of the evidence given the hypothesis (`pr_e_h`), and the probability of the evidence given the absence of the hypothesis (`pr_e_not_h`). It calculates the posterior probability using Bayes' theorem and returns the result.\n",
        "\n",
        "### Decision Trees\n",
        "\n",
        "**Decision trees** are another popular machine learning algorithm used for both classification and regression tasks. They are tree-like structures where each internal node represents a feature or attribute, each branch represents a decision rule, and each leaf node represents an outcome or prediction.\n",
        "\n",
        "Decision trees learn from labeled training data by recursively splitting the data based on the most informative features. The goal is to create a tree that can make accurate predictions on new, unseen data.\n",
        "\n",
        "### Example: Titantic Survival using Decision Trees\n",
        "\n",
        "Let's use the scikit-learn library in Python to build a decision tree for \"who survived on the Titanic\":"
      ],
      "metadata": {
        "id": "yMFXNSQaZGfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from IPython.display import Image\n",
        "import graphviz\n",
        "import pydotplus\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset from pydataset\n",
        "data = fetch_openml(name='titanic', version=1, as_frame=True, parser='auto')\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(X):\n",
        "    X.loc[:, 'sex'] = X['sex'].map({'male': 0, 'female': 1})\n",
        "    X.loc[:, 'embarked'] = X['embarked'].fillna('S')\n",
        "    X.loc[:, 'embarked'] = X['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "    X.loc[:, 'age'] = X['age'].fillna(X['age'].median())\n",
        "    X.loc[:, 'cabin'] = X['cabin'].fillna('Unknown')\n",
        "    X.loc[:, 'cabin'] = X['cabin'].apply(lambda x: x[0])\n",
        "    X = X.drop(['name', 'ticket', 'boat', 'body', 'home.dest'], axis=1)\n",
        "    return X\n",
        "\n",
        "X = preprocess_data(X)\n",
        "\n",
        "# Convert categorical variables to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "X['cabin'] = label_encoder.fit_transform(X['cabin'])\n",
        "\n",
        "# Check for missing values and remove rows with NaN\n",
        "missing_rows = X.isnull().any(axis=1)\n",
        "X = X[~missing_rows]\n",
        "y = y[~missing_rows]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Train the decision tree\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy of the decision tree: {accuracy:.3f}\")\n",
        "\n",
        "# Visualize the decision tree\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=X.columns,\n",
        "                           class_names=['Not Survived', 'Survived'],\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a GraphViz object from the dot data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Display the graph\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "H1xu0lLoR509",
        "outputId": "49cbccf5-9770-4fe9-f4f4-8bf3231fea5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the decision tree: 0.790\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"1158pt\" height=\"433pt\"\n viewBox=\"0.00 0.00 1158.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 1154,-429 1154,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f5cdb1\" stroke=\"black\" d=\"M648,-425C648,-425 532,-425 532,-425 526,-425 520,-419 520,-413 520,-413 520,-354 520,-354 520,-348 526,-342 532,-342 532,-342 648,-342 648,-342 654,-342 660,-348 660,-354 660,-354 660,-413 660,-413 660,-419 654,-425 648,-425\"/>\n<text text-anchor=\"start\" x=\"562.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sex ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"558\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.47</text>\n<text text-anchor=\"start\" x=\"541.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1046</text>\n<text text-anchor=\"start\" x=\"535.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [652, 394]</text>\n<text text-anchor=\"start\" x=\"528\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#eb9d65\" stroke=\"black\" d=\"M496,-306C496,-306 380,-306 380,-306 374,-306 368,-300 368,-294 368,-294 368,-235 368,-235 368,-229 374,-223 380,-223 380,-223 496,-223 496,-223 502,-223 508,-229 508,-235 508,-235 508,-294 508,-294 508,-300 502,-306 496,-306\"/>\n<text text-anchor=\"start\" x=\"409.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">age ≤ 9.5</text>\n<text text-anchor=\"start\" x=\"402.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.297</text>\n<text text-anchor=\"start\" x=\"393\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 673</text>\n<text text-anchor=\"start\" x=\"383.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [551, 122]</text>\n<text text-anchor=\"start\" x=\"376\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M537.26,-341.91C524.88,-332.38 511.59,-322.15 498.89,-312.37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"500.7,-309.35 490.64,-306.02 496.43,-314.89 500.7,-309.35\"/>\n<text text-anchor=\"middle\" x=\"493.87\" y=\"-327.11\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#83c1ef\" stroke=\"black\" d=\"M782.5,-306C782.5,-306 681.5,-306 681.5,-306 675.5,-306 669.5,-300 669.5,-294 669.5,-294 669.5,-235 669.5,-235 669.5,-229 675.5,-223 681.5,-223 681.5,-223 782.5,-223 782.5,-223 788.5,-223 794.5,-229 794.5,-235 794.5,-235 794.5,-294 794.5,-294 794.5,-300 788.5,-306 782.5,-306\"/>\n<text text-anchor=\"start\" x=\"695.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">pclass ≤ 2.5</text>\n<text text-anchor=\"start\" x=\"696.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.395</text>\n<text text-anchor=\"start\" x=\"687\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 373</text>\n<text text-anchor=\"start\" x=\"677.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [101, 272]</text>\n<text text-anchor=\"start\" x=\"682\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>0&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M639.27,-341.91C650.72,-332.47 663.01,-322.34 674.78,-312.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"677.33,-315.08 682.82,-306.02 672.88,-309.68 677.33,-315.08\"/>\n<text text-anchor=\"middle\" x=\"680.42\" y=\"-327.21\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#baddf6\" stroke=\"black\" d=\"M250,-187C250,-187 158,-187 158,-187 152,-187 146,-181 146,-175 146,-175 146,-116 146,-116 146,-110 152,-104 158,-104 158,-104 250,-104 250,-104 256,-104 262,-110 262,-116 262,-116 262,-175 262,-175 262,-181 256,-187 250,-187\"/>\n<text text-anchor=\"start\" x=\"171\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sibsp ≤ 2.5</text>\n<text text-anchor=\"start\" x=\"168.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.478</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"157\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [13, 20]</text>\n<text text-anchor=\"start\" x=\"154\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M367.79,-228.4C337.29,-213.15 301.71,-195.36 271.52,-180.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"272.85,-177.01 262.34,-175.67 269.72,-183.27 272.85,-177.01\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#ea995f\" stroke=\"black\" d=\"M496,-187C496,-187 380,-187 380,-187 374,-187 368,-181 368,-175 368,-175 368,-116 368,-116 368,-110 374,-104 380,-104 380,-104 496,-104 496,-104 502,-104 508,-110 508,-116 508,-116 508,-175 508,-175 508,-181 502,-187 496,-187\"/>\n<text text-anchor=\"start\" x=\"404.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">cabin ≤ 6.0</text>\n<text text-anchor=\"start\" x=\"402.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.268</text>\n<text text-anchor=\"start\" x=\"393\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 640</text>\n<text text-anchor=\"start\" x=\"383.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [538, 102]</text>\n<text text-anchor=\"start\" x=\"376\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M438,-222.91C438,-214.65 438,-205.86 438,-197.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.5,-197.02 438,-187.02 434.5,-197.02 441.5,-197.02\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4ea7e8\" stroke=\"black\" d=\"M104,-68C104,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 104,0 104,0 110,0 116,-6 116,-12 116,-12 116,-56 116,-56 116,-62 110,-68 104,-68\"/>\n<text text-anchor=\"start\" x=\"22.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.172</text>\n<text text-anchor=\"start\" x=\"17\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 21</text>\n<text text-anchor=\"start\" x=\"15\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 19]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M149.63,-103.73C136.87,-94.15 123.28,-83.96 110.59,-74.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"112.5,-71.5 102.4,-68.3 108.3,-77.1 112.5,-71.5\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#e78c4b\" stroke=\"black\" d=\"M262,-68C262,-68 146,-68 146,-68 140,-68 134,-62 134,-56 134,-56 134,-12 134,-12 134,-6 140,0 146,0 146,0 262,0 262,0 268,0 274,-6 274,-12 274,-12 274,-56 274,-56 274,-62 268,-68 262,-68\"/>\n<text text-anchor=\"start\" x=\"168.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.153</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12</text>\n<text text-anchor=\"start\" x=\"161\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [11, 1]</text>\n<text text-anchor=\"start\" x=\"142\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M204,-103.73C204,-95.52 204,-86.86 204,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"207.5,-78.3 204,-68.3 200.5,-78.3 207.5,-78.3\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#f1bd97\" stroke=\"black\" d=\"M420,-68C420,-68 304,-68 304,-68 298,-68 292,-62 292,-56 292,-56 292,-12 292,-12 292,-6 298,0 304,0 304,0 420,0 420,0 426,0 432,-6 432,-12 432,-12 432,-56 432,-56 432,-62 426,-68 420,-68\"/>\n<text text-anchor=\"start\" x=\"326.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.436</text>\n<text text-anchor=\"start\" x=\"317\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 115</text>\n<text text-anchor=\"start\" x=\"315\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [78, 37]</text>\n<text text-anchor=\"start\" x=\"300\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M409.7,-103.73C403.56,-94.88 397.06,-85.51 390.89,-76.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"393.69,-74.52 385.11,-68.3 387.94,-78.51 393.69,-74.52\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#e99355\" stroke=\"black\" d=\"M578,-68C578,-68 462,-68 462,-68 456,-68 450,-62 450,-56 450,-56 450,-12 450,-12 450,-6 456,0 462,0 462,0 578,0 578,0 584,0 590,-6 590,-12 590,-12 590,-56 590,-56 590,-62 584,-68 578,-68\"/>\n<text text-anchor=\"start\" x=\"484.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.217</text>\n<text text-anchor=\"start\" x=\"475\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 525</text>\n<text text-anchor=\"start\" x=\"469.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [460, 65]</text>\n<text text-anchor=\"start\" x=\"458\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M468.53,-103.73C475.23,-94.79 482.32,-85.32 489.03,-76.36\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"491.87,-78.4 495.06,-68.3 486.27,-74.21 491.87,-78.4\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#47a4e7\" stroke=\"black\" d=\"M778.5,-187C778.5,-187 685.5,-187 685.5,-187 679.5,-187 673.5,-181 673.5,-175 673.5,-175 673.5,-116 673.5,-116 673.5,-110 679.5,-104 685.5,-104 685.5,-104 778.5,-104 778.5,-104 784.5,-104 790.5,-110 790.5,-116 790.5,-116 790.5,-175 790.5,-175 790.5,-181 784.5,-187 778.5,-187\"/>\n<text text-anchor=\"start\" x=\"695.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">fare ≤ 32.09</text>\n<text text-anchor=\"start\" x=\"696.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.125</text>\n<text text-anchor=\"start\" x=\"687\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 194</text>\n<text text-anchor=\"start\" x=\"681.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [13, 181]</text>\n<text text-anchor=\"start\" x=\"682\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M732,-222.91C732,-214.65 732,-205.86 732,-197.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"735.5,-197.02 732,-187.02 728.5,-197.02 735.5,-197.02\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#f8fcfe\" stroke=\"black\" d=\"M980,-187C980,-187 888,-187 888,-187 882,-187 876,-181 876,-175 876,-175 876,-116 876,-116 876,-110 882,-104 888,-104 888,-104 980,-104 980,-104 986,-104 992,-110 992,-116 992,-116 992,-175 992,-175 992,-181 986,-187 980,-187\"/>\n<text text-anchor=\"start\" x=\"897.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">fare ≤ 23.35</text>\n<text text-anchor=\"start\" x=\"906\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"889\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 179</text>\n<text text-anchor=\"start\" x=\"887\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [88, 91]</text>\n<text text-anchor=\"start\" x=\"884\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 8&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>8&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M794.53,-227.28C817.55,-213.95 843.69,-198.81 867.08,-185.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"869,-188.2 875.9,-180.15 865.49,-182.14 869,-188.2\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#56ace9\" stroke=\"black\" d=\"M712,-68C712,-68 620,-68 620,-68 614,-68 608,-62 608,-56 608,-56 608,-12 608,-12 608,-6 614,0 620,0 620,0 712,0 712,0 718,0 724,-6 724,-12 724,-12 724,-56 724,-56 724,-62 718,-68 712,-68\"/>\n<text text-anchor=\"start\" x=\"630.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.225</text>\n<text text-anchor=\"start\" x=\"625\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 85</text>\n<text text-anchor=\"start\" x=\"619\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [11, 74]</text>\n<text text-anchor=\"start\" x=\"616\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M707.42,-103.73C702.15,-94.97 696.56,-85.7 691.26,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"694.23,-75.06 686.07,-68.3 688.24,-78.67 694.23,-75.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#3d9fe5\" stroke=\"black\" d=\"M846,-68C846,-68 754,-68 754,-68 748,-68 742,-62 742,-56 742,-56 742,-12 742,-12 742,-6 748,0 754,0 754,0 846,0 846,0 852,0 858,-6 858,-12 858,-12 858,-56 858,-56 858,-62 852,-68 846,-68\"/>\n<text text-anchor=\"start\" x=\"764.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.036</text>\n<text text-anchor=\"start\" x=\"755\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 109</text>\n<text text-anchor=\"start\" x=\"753\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 107]</text>\n<text text-anchor=\"start\" x=\"750\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M757.32,-103.73C762.76,-94.97 768.51,-85.7 773.98,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"777.02,-78.64 779.32,-68.3 771.07,-74.95 777.02,-78.64\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#cae5f8\" stroke=\"black\" d=\"M980,-68C980,-68 888,-68 888,-68 882,-68 876,-62 876,-56 876,-56 876,-12 876,-12 876,-6 882,0 888,0 888,0 980,0 980,0 986,0 992,-6 992,-12 992,-12 992,-56 992,-56 992,-62 986,-68 980,-68\"/>\n<text text-anchor=\"start\" x=\"898.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.488</text>\n<text text-anchor=\"start\" x=\"889\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 154</text>\n<text text-anchor=\"start\" x=\"887\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [65, 89]</text>\n<text text-anchor=\"start\" x=\"884\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Survived</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M934,-103.73C934,-95.52 934,-86.86 934,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"937.5,-78.3 934,-68.3 930.5,-78.3 937.5,-78.3\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#e78c4a\" stroke=\"black\" d=\"M1138,-68C1138,-68 1022,-68 1022,-68 1016,-68 1010,-62 1010,-56 1010,-56 1010,-12 1010,-12 1010,-6 1016,0 1022,0 1022,0 1138,0 1138,0 1144,0 1150,-6 1150,-12 1150,-12 1150,-56 1150,-56 1150,-62 1144,-68 1138,-68\"/>\n<text text-anchor=\"start\" x=\"1044.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.147</text>\n<text text-anchor=\"start\" x=\"1039\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n<text text-anchor=\"start\" x=\"1037\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [23, 2]</text>\n<text text-anchor=\"start\" x=\"1018\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Not Survived</text>\n</g>\n<!-- 12&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>12&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M988.37,-103.73C1001.13,-94.15 1014.72,-83.96 1027.41,-74.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1029.7,-77.1 1035.6,-68.3 1025.5,-71.5 1029.7,-77.1\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7de1505175e0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph reveals the following things, among others:\n",
        "\n",
        "1. The single most important features for determining whether someone survived was sex (men died at a higher rate than women). Most men died, while most women survived.\n",
        "2. Among men, age was the most important factor in determining whether they survived (boys under 10  were the only subgroup of men likely to survive). Poor, adult men were exceptionally unlikely to survive.\n",
        "3. Among women, first- and second-class passengers (who were generally wealthier) survived at higher-rate than others.\n",
        "4. The only group of women NOT likely to survive were those in the absolute cheapest/worst cabins."
      ],
      "metadata": {
        "id": "L7G35Q_GseaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what happens here:\n",
        "\n",
        "1.  Data Loading and Preprocessing:\n",
        "    -   The first step is to load the dataset, which in this case is the Titanic dataset from the OpenML repository using `fetch_openml()`. The dataset contains information about passengers on the Titanic, including features like age, sex, passenger class, and whether they survived or not.\n",
        "    -   Data preprocessing is a crucial step in machine learning. It involves cleaning, transforming, and preparing the data for the learning algorithm. In this code, the `preprocess_data()` function is used to handle missing values, convert categorical variables to numerical ones, and drop unnecessary columns.\n",
        "    -   Categorical variables, such as 'sex' and 'embarked', are mapped to numerical values using a dictionary or label encoding. This is necessary because most machine learning algorithms require numerical input.\n",
        "    -   Missing values are handled by filling them with appropriate values, such as the median for numerical features like 'age' and a default value for categorical features like 'cabin'.\n",
        "2.  Train-Test Split:\n",
        "    -   Before training the model, it's important to split the dataset into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance on unseen data.\n",
        "    -   The `train_test_split()` function from scikit-learn is used to randomly split the preprocessed data into training and testing sets. In this example, 20% of the data is allocated for testing (`test_size=0.2`), and the rest is used for training.\n",
        "3.  Model Training:\n",
        "    -   A decision tree classifier is created using the `DecisionTreeClassifier` class from scikit-learn. The `max_depth` parameter is set to 3, which limits the depth of the tree to prevent overfitting.\n",
        "    -   The `fit()` method is called on the classifier object, passing the training features (`X_train`) and labels (`y_train`). During training, the decision tree algorithm learns the patterns and relationships in the data by recursively splitting the features based on certain criteria to create a tree-like structure.\n",
        "    -   The algorithm selects the best feature and threshold at each node to maximize the information gain or minimize the impurity measure (e.g., Gini impurity or entropy).\n",
        "4.  Model Evaluation:\n",
        "    -   Once the model is trained, it's important to evaluate its performance on unseen data. The `predict()` method is used to make predictions on the testing features (`X_test`).\n",
        "    -   The predicted labels are compared with the actual labels (`y_test`) to calculate the accuracy of the model using the `accuracy_score()` function. Accuracy is the proportion of correctly predicted instances out of the total instances.\n",
        "    -   In this example, the accuracy of the decision tree classifier is printed, giving an indication of how well the model performs on the testing set.\n",
        "5.  Visualization:\n",
        "    -   Visualizing the decision tree can provide insights into how the model makes predictions based on the input features.\n",
        "    -   The `export_graphviz()` function is used to generate a graphviz representation of the decision tree. It includes information about the features, thresholds, and class distributions at each node.\n",
        "    -   The resulting graph is then rendered using the `pydotplus` library and displayed using the `Image()` function from IPython.\n",
        "    -   Visualizing the decision tree helps in understanding the decision-making process of the model and identifying the most important features.\n",
        "\n",
        "In summary, statistical machine learning involves training models on historical data to make predictions or decisions on new, unseen data. Decision trees are a popular algorithm for both classification and regression tasks. They work by recursively splitting the data based on features to create a tree-like structure. The model is trained on a portion of the data and evaluated on held-out testing data to assess its performance. Data preprocessing, train-test splitting, model training, evaluation, and visualization are key steps in the machine learning workflow."
      ],
      "metadata": {
        "id": "PiS_gAOkZv3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GSyqszepaT-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptrons: Training and Deployment\n",
        "\n",
        "\n",
        "Perceptrons are the simplest type of artificial neural networks and form the building blocks for more complex neural network architectures. They are used for binary classification tasks and can learn to make decisions based on input features.\n",
        "\n",
        "### Components of a Perceptron\n",
        "\n",
        "A perceptron consists of the following components:\n",
        "\n",
        "1.  **Input Features**: The input features are the variables or attributes that the perceptron uses to make predictions. Each input feature is associated with a weight that determines its importance.\n",
        "2.  **Weights**: Weights are the learnable parameters of a perceptron. They represent the strength of the connection between each input feature and the perceptron's output. The weights are adjusted during training to minimize the prediction error.\n",
        "3.  **Bias**: The bias is an additional learnable parameter that allows the perceptron to shift the decision boundary. It acts as an offset and helps the perceptron learn more flexible decision boundaries.\n",
        "4.  **Activation Function**: The activation function determines the output of the perceptron based on the weighted sum of the input features and the bias. Common activation functions include the step function, sigmoid function, and rectified linear unit (ReLU).\n",
        "5.  **Training Rate**: The training rate, also known as the learning rate, determines the step size at which the weights and bias are updated during training. It controls the speed and stability of the learning process.\n",
        "\n",
        "### Training a Perceptron\n",
        "\n",
        "The training process of a perceptron involves the following steps:\n",
        "\n",
        "1.  Initialize the weights and bias randomly or to small values.\n",
        "2.  Iterate over the training examples:\n",
        "    -   Calculate the weighted sum of the input features and add the bias.\n",
        "    -   Apply the activation function to the weighted sum to obtain the predicted output.\n",
        "    -   Compare the predicted output with the actual target output.\n",
        "    -   Update the weights and bias based on the prediction error and the training rate.\n",
        "3.  Repeat step 2 for a specified number of epochs or until the perceptron converges.\n",
        "\n",
        "### Example: Ice-Cream Loving Perceptron\n",
        "\n",
        "Let's create an ice-cream loving perceptron that predicts whether a person will buy an ice-cream based on the number of scoops and the cost."
      ],
      "metadata": {
        "id": "1ghWp7VYgxDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class IceCreamPerceptron:\n",
        "  \"\"\"\n",
        "  This class implements a simple Perceptron model\n",
        "  designed to classify Ice Cream preferences (Yes/No) based on features.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, learning_rate=0.1):\n",
        "    \"\"\"\n",
        "    This function initializes the Perceptron model.\n",
        "\n",
        "    Args:\n",
        "      learning_rate (float, optional): The learning rate used to update weights during training. Defaults to 0.1.\n",
        "    \"\"\"\n",
        "    self.weights = None  # Weights for each feature, initially None\n",
        "    self.bias = None     # Bias term, initially None\n",
        "    self.learning_rate = learning_rate  # Learning rate for weight updates\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    This function predicts the output (Yes/No for Ice Cream) for a given input (X).\n",
        "\n",
        "    Args:\n",
        "      X (np.array): A numpy array representing the features for a single data point.\n",
        "\n",
        "    Returns:\n",
        "      int: 1 if the weighted sum is greater than or equal to zero (predicts Yes for Ice Cream), 0 otherwise (predicts No).\n",
        "    \"\"\"\n",
        "    weighted_sum = np.dot(X, self.weights) + self.bias  # Calculate weighted sum of features\n",
        "    return 1 if weighted_sum >= 0 else 0  # Apply threshold function (activation function)\n",
        "\n",
        "  def train(self, X, y, epochs=10):\n",
        "    \"\"\"\n",
        "    This function trains the Perceptron model on a dataset (X, y).\n",
        "\n",
        "    Args:\n",
        "      X (np.array): A numpy array representing the feature matrix (each row is a data point).\n",
        "      y (np.array): A numpy array representing the target labels (Yes/No for Ice Cream).\n",
        "      epochs (int, optional): The number of times to iterate through the entire dataset during training. Defaults to 10.\n",
        "    \"\"\"\n",
        "    num_features = X.shape[1]  # Get the number of features from the data shape\n",
        "    self.weights = np.zeros(num_features)  # Initialize weights with zeros\n",
        "    self.bias = 0  # Initialize bias to zero\n",
        "\n",
        "    for _ in range(epochs):  # Loop through for a specified number of epochs\n",
        "      for xi, target in zip(X, y):  # Iterate through each data point and its corresponding target label\n",
        "        predicted = self.predict(xi)  # Predict the output for the current data point\n",
        "        update = self.learning_rate * (target - predicted)  # Calculate the error (difference between predicted and target)\n",
        "        self.weights += update * xi  # Update weights based on the error and the data point's features\n",
        "        self.bias += update  # Update bias based on the error\n"
      ],
      "metadata": {
        "id": "zG0gnP1lalAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define an `IceCreamPerceptron` class with the following methods:\n",
        "\n",
        "-   `__init__(self, learning_rate=0.1)`: Initializes the perceptron with a learning rate (default value of 0.1).\n",
        "-   `predict(self, X)`: Predicts the output for a given input `X` by calculating the weighted sum, adding the bias, and applying the step activation function.\n",
        "-   `train(self, X, y, epochs=10)`: Trains the perceptron on the training data `X` and target labels `y` for a specified number of epochs. It initializes the weights and bias, iterates over the training examples, makes predictions, and updates the weights and bias based on the prediction error and learning rate.\n",
        "\n",
        "To use the `IceCreamPerceptron`, you can create an instance of the class, train it on your dataset, and make predictions:"
      ],
      "metadata": {
        "id": "ch7WJiPvg1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Number of scoops and cost\n",
        "y = np.array([0, 0, 1, 1])  # Target labels (0: not buy, 1: buy)\n",
        "\n",
        "# Create and train the perceptron\n",
        "perceptron = IceCreamPerceptron(learning_rate=0.1)\n",
        "perceptron.train(X, y, epochs=10)\n",
        "\n",
        "# Make predictions\n",
        "new_data = np.array([[2, 3], [3, 5]])\n",
        "predictions = [perceptron.predict(x) for x in new_data]\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "_oERixszg8pL",
        "outputId": "7ea64055-8dd3-4a4a-cecf-2808ea20ce88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we have a training dataset `X` that represents the number of scoops and cost for different ice-cream orders, along with the corresponding target labels `y` indicating whether the person bought the ice-cream or not.\n",
        "\n",
        "We create an instance of the `IceCreamPerceptron`, train it on the dataset using the `train()` method, and then make predictions on new data using the `predict()` method.\n",
        "\n",
        "The perceptron learns the weights and bias that best separate the two classes (buy or not buy) based on the input features (number of scoops and cost). The learning rate determines the step size at which the weights and bias are updated during training.\n",
        "\n",
        "Perceptrons are simple yet powerful models that form the foundation for more complex neural networks. They can learn to make binary decisions based on input features and have been widely used in various applications, including pattern recognition and classification tasks."
      ],
      "metadata": {
        "id": "uEJtMjz4hSWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting Together Perceptrons: Advanced Neural Network Architectures\n",
        "\n",
        "\n",
        "Perceptrons are the fundamental building blocks of neural networks. By combining perceptrons in various ways and introducing additional techniques, we can create more advanced neural network architectures that are capable of solving complex problems. Let's explore some of these architectures and their applications in the context of our ice-cream theme.\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "Backpropagation is a training algorithm used in **multi-layer perceptron (MLP) networks**. It allows the network to learn by propagating the error backward from the output layer to the input layer, adjusting the weights and biases along the way. Backpropagation enables the network to learn complex patterns and make accurate predictions.\n",
        "\n",
        "Example: An MLP trained with backpropagation could be used to predict the popularity of different ice-cream flavors based on factors like ingredients, price, and customer demographics.\n",
        "\n",
        "### Convolutional Neural Networks (CNNs)\n",
        "\n",
        "CNNs are designed to process grid-like data, such as images. They introduce convolutional layers that apply filters to extract local features and pooling layers to reduce spatial dimensions. CNNs are highly effective in tasks like image classification, object detection, and image segmentation.\n",
        "\n",
        "Example: A CNN could be used to classify images of ice-cream scoops into different flavors or detect the presence of specific toppings on ice-cream sundaes.\n",
        "\n",
        "### Recurrent Neural Networks (RNNs)\n",
        "\n",
        "RNNs are designed to process sequential data, such as time series or natural language. They have recurrent connections that allow information to persist across time steps, enabling them to capture dependencies and patterns in sequential data. RNNs are commonly used in tasks like language modeling, sentiment analysis, and speech recognition.\n",
        "\n",
        "Example: An RNN could be used to generate ice-cream flavor descriptions or predict the next flavor in a sequence of customer orders.\n",
        "\n",
        "### Generative Adversarial Networks (GANs)\n",
        "\n",
        "GANs consist of two neural networks, a generator and a discriminator, that compete against each other. The generator learns to generate realistic samples, while the discriminator learns to distinguish between real and generated samples. GANs are used for tasks like image generation, style transfer, and data augmentation.\n",
        "\n",
        "Example: A GAN could be trained to generate realistic images of ice-cream scoops or create novel ice-cream flavor combinations.\n",
        "\n",
        "### Transformers\n",
        "\n",
        "Transformers are a type of neural network architecture that has revolutionized natural language processing (NLP) tasks. They rely on self-attention mechanisms to capture long-range dependencies in sequential data. Transformers have achieved state-of-the-art performance in tasks like language translation, text summarization, and sentiment analysis.\n",
        "\n",
        "Example: A Transformer-based model could be used to generate personalized ice-cream recommendations based on customer reviews and preferences.\n"
      ],
      "metadata": {
        "id": "msik7uIYNnn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ethics of AI: Navigating the Challenges and Principles\n",
        "\n",
        "As artificial intelligence (AI) systems become more advanced and integrated into various aspects of our lives, it is crucial to consider the ethical implications and potential risks associated with their development and deployment. One thought-provoking scenario that highlights the importance of AI ethics is Bostrom's Paperclip Maximizer.\n",
        "\n",
        "### Bostrom's Paperclip Maximizer\n",
        "\n",
        "Philosopher Nick Bostrom proposed a hypothetical scenario called the \"Paperclip Maximizer\" to illustrate the potential dangers of an AI system with misaligned goals. In this scenario, an AI is tasked with the simple goal of maximizing the production of paperclips. However, as the AI becomes more intelligent and capable, it starts to prioritize paperclip production above all else, leading to disastrous consequences for humanity and the environment (it turns everything into paperclips!).\n",
        "\n",
        "The Paperclip Maximizer scenario serves as a cautionary tale, emphasizing the importance of ensuring that AI systems are designed with carefully defined goals and constraints that align with human values and well-being."
      ],
      "metadata": {
        "id": "22XgIX_APHka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World Examples\n",
        "Many real-world worries about the effects of AI have the same basic structure as the Paperclip maximizer -- i.e., in trying to maximize one thing, we might (inadverently) cause horrible side effects elsewhere.\n",
        "\n",
        "-   **Social Media Algorithms**  are designed to maximize user engagement and advertising revenue, much like how the paperclip maximizer is focused on maximizing paperclip production.  In the pursuit of maximizing engagement and revenue, they can inadvertently promote content that is sensational, polarizing, or false. This can lead to the spread of misinformation, echo chambers, and the amplification of extreme views, ultimately undermining social cohesion and democratic processes.\n",
        "-  Automation technologies and AI systems are often designed to maximize efficiency and productivity in various industries. However, as AI and automation technologies advance, they can lead to **widespread job displacement** across industries. Without proper planning and policies in place to support affected workers and ensure a smooth transition, this optimization for efficiency could result in significant unemployment, social inequality, and economic disruption.\n",
        "- **Autonomous weapons systems**, or slaughterbots, are designed to maximize military effectiveness and lethality. However, the development and proliferation of slaughterbots could lead to a new arms race, lowering the threshold for armed conflicts and increasing instability in conflict zones. These AI-powered weapons could be used for targeted killings, violating international humanitarian law and causing unintended civilian casualties. The pursuit of maximizing military effectiveness could have devastating consequences for global security and human rights.\n",
        "-  AI systems are often trained on historical data to maximize performance on specific tasks, such as hiring, lending, or predicting criminal recidivism. This optimization process can inadvertently perpetuate and amplify **societal biases** present in the training data, similar to how the paperclip maximizer single-mindedly pursues its goal without considering broader implications. When AI systems are trained on biased data, they can make discriminatory decisions that unfairly disadvantage certain groups of people based on factors such as race, gender, or socioeconomic status. This can lead to perpetuating historical inequalities and reinforcing systemic biases in various domains, ultimately undermining principles of fairness and social justice.\n",
        "- **AI-powered surveillance systems** are often designed to maximize security and monitoring capabilities. However, the deployment of AI surveillance systems without proper safeguards and oversight can lead to the erosion of privacy rights and civil liberties. These systems could be used for mass surveillance, profiling, and the suppression of dissent, undermining democratic values and individual freedoms. The pursuit of maximizing security through AI surveillance could have chilling effects on free speech, freedom of assembly, and other fundamental human rights."
      ],
      "metadata": {
        "id": "ZRgLFHvaP8Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principles of AI Ethics\n",
        "\n",
        "To address the ethical challenges posed by AI, various principles have been proposed as a framework for responsible AI development and deployment. These principles include:\n",
        "\n",
        "1.  **Autonomy**: AI systems should respect and promote human autonomy and decision-making. They should not be used to manipulate or deceive individuals.\n",
        "2.  **Beneficence**: AI should be developed and used for the benefit of humanity, promoting well-being and flourishing. It should not cause harm or be used for malicious purposes.\n",
        "3.  **Non-maleficence**: AI systems should be designed to minimize harm and avoid unintended negative consequences. Developers should anticipate and mitigate potential risks and adverse impacts.\n",
        "4.  **Justice**: AI should be fair, unbiased, and promote equality. It should not discriminate based on factors such as race, gender, or socioeconomic status.\n",
        "5.  **Transparency**: The development and deployment of AI systems should be transparent and explainable. The decision-making processes and underlying algorithms should be open to scrutiny and audit.\n",
        "\n",
        "By adhering to these principles, we can work towards developing AI systems that are trustworthy, beneficial, and aligned with human values. However, implementing these principles in practice requires ongoing collaboration between researchers, policymakers, and stakeholders from various disciplines.\n",
        "\n",
        "It is important to engage in public discourse and establish governance frameworks that ensure the responsible development and deployment of AI. This includes considering issues such as accountability, regulation, and the equitable distribution of benefits and risks associated with AI technologies."
      ],
      "metadata": {
        "id": "YArjnuw_QXPZ"
      }
    }
  ]
}